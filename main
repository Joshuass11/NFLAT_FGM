import os
import torch

from torch.optim.lr_scheduler import LambdaLR
#引入NFLAT类
from models.NFLAT import NFLAT
from fastNLP import Trainer, GradientClipCallback, WarmupCallback
from torch import optim, nn
from fastNLP import SpanFPreRecMetric, BucketSampler, LRScheduler, AccuracyMetric

import argparse
#两个函数
from modules.utils import set_rng_seed, MyEvaluateCallback
#load_yangjie_rich_pretrain_word_list用于存放读取的词
from utils.load_data import load_data, equip_chinese_ner_with_lexicon, load_yangjie_rich_pretrain_word_list
from utils.paths import *

import sys
#注意这里导入完以后没有用到transformer
sys.setrecursionlimit(100000)

#注意，这个mian函数中用到了刚刚导入的各个函数以及NFLAT类，但是都相互平行，先设置参数，再填入函数
def main():
    device = "cuda" if torch.cuda.is_available() else "cpu"
    refresh_data = False
    parser = argparse.ArgumentParser()
    # 注释传入变量,统一在形参输入
    parser.add_argument('--dataset', type=str, default='weibo', choices=['weibo', 'resume', 'ontonotes', 'msra'])
    parser.add_argument('--lexicon_name', default='yj', choices=['lk', 'yj', 'tx'])
    parser.add_argument('--only_lexicon_in_train', default=False)
  #在这里找到weibo的测试数据
    parser.add_argument('--label_type',  default='ALL', help='NE|NM|ALL')

    args = parser.parse_args()
    dataset = args.dataset
#！！！不预先设定
    pos_embed = None

    batch_size = 10
    warmup_steps = 0.2
    after_norm = 1
    # ！！！
    model_type = 'transformer'
    #是否对嵌入进行归一化操作，`True` 表示进行归一化
    normalize_embed = True

    char_embed_dropout = 0.5
    word_embed_dropout = 0.5
    momentum = 0.9
#fc_dropout = 0.3: 设置了全连接层之间的 dropout 概率为 0.3。

#attn_dropout = 0: 设置了注意力机制中的 dropout 概率为 0。
    dropout = 0.2
    fc_dropout = 0.3
    attn_dropout = 0
    # 在这个代码中可能表示着一种位置编码的融合方式。！！！
    four_pos_fusion = 'ff_two'
#设置了CRF层的学习率为1。
    crf_lr = 1
#这里scale改成true
    before = False
    scale = False
    k_proj = True
    softmax_axis = -1
    is_less_head = 1
    use_bigram = 1

    seed = 2022
    #第一个用到的函数
    set_rng_seed(seed)
    # 将注意力机制的 dropout 概率设为 `0`，
   #这里注意一下
    attn_type = 'adatrans'
#注意力机制层数是1，包括interformer和transformer
    num_layers = 1

    n_epochs = 100

    if dataset == 'resume':
        attn_dropout = 0
        batch_size = 16
        char_embed_dropout = 0.6
        dropout = 0.2
        fc_dropout = 0
        is_less_head = 1
        lr = 0.002
        n_epochs = 50
        use_bigram = 1
        warmup_steps = 0.05
        word_embed_dropout = 0.5
        n_heads = 8
        head_dims = 16
    elif dataset == 'weibo':
        attn_dropout = 0.2
        batch_size = 16
        char_embed_dropout = 0.6
        dropout = 0
        fc_dropout = 0.2
        is_less_head = 2
        lr = 0.003
        use_bigram = 1
        warmup_steps = 0.4
        word_embed_dropout = 0.4
        n_heads = 12
        head_dims = 16
    elif dataset == 'ontonotes':
        batch_size = 8
        char_embed_dropout = 0.4
        dropout = 0.2
        fc_dropout = 0.2
        attn_dropout = 0.2
        is_less_head = 2
        lr = 0.0008#学习率
        use_bigram = 1
        warmup_steps = 0.1
        word_embed_dropout = 0.4
        n_heads = 8
        head_dims = 32
    elif dataset == 'msra':
        attn_dropout = 0
        #原来是16
        batch_size = 2
        char_embed_dropout = 0.4
        dropout = 0.2
        fc_dropout = 0
        is_less_head = 1
        lr = 0.002
        use_bigram = 1
        warmup_steps = 0.2
        word_embed_dropout = 0.4
        #使用了8个注意力头，每个头的维度是32
        n_heads = 16
        #之前是32
        head_dims = 1
#在初始化模型参数时，使用均匀分布来生成初始值。
    args.init = 'uniform'
#！！
    encoding_type = 'bio'
    if args.dataset == 'ontonotes':
        encoding_type = 'bmeso'

#模型维度
    d_model = n_heads * head_dims
    dim_feedforward = int(2 * d_model)
    # 加载数据集，用到load_data里引进的函数,only_train_min_freq=1值是1，覆盖掉定义的True
    datasets, vocabs, embeddings = load_data(dataset, index_token=False, char_min_freq=1, bigram_min_freq=1,
                                             only_train_min_freq=1, char_dropout=0.01, label_type=args.label_type,
                                             refresh_data=refresh_data)
#这里的三个路径都是本来有的，不是自己创建的，是词典，除了yj的第二个参数的数据集是自己做的
    if args.lexicon_name == 'lk':
        word_path = lk_word_path
        word_char_mix_embedding_path = lk_word_path
        lex = 'lk'
    elif args.lexicon_name == 'tx':
        word_path = tencet_word_path
        word_char_mix_embedding_path = tencet_word_path
        lex = 'tx'
    else:
        word_path = yangjie_rich_pretrain_word_path
        word_char_mix_embedding_path = yangjie_rich_pretrain_char_and_word_path
        lex = 'yj'
#这里也是来自load_data,参数上面都定义了，即词典列表，wordpath就是tencent什么的
    w_list = load_yangjie_rich_pretrain_word_list(word_path,
                                                  _refresh=refresh_data,
                                                  _cache_fp='cache/{}'.format(args.lexicon_name))
#这里也和weibo有关,是NE或NM就是把label_type给type，否则就不给
    type = args.label_type if args.label_type != 'ALL' else ''
    cache_name = os.path.join('cache', ('dataset_{}_lex_{}{}'.format(
        args.dataset, lex, type)))#缓存文件的路径
    #这里的yangjie_rich_pretrain_word_path在换词典时要换一下
    datasets, vocabs, embeddings = equip_chinese_ner_with_lexicon(datasets, vocabs, embeddings,
                                                                  w_list, yangjie_rich_pretrain_word_path,
                                                                  _refresh=refresh_data, _cache_fp=cache_name,
                                                                  only_lexicon_in_train=args.only_lexicon_in_train,
                                                                  word_char_mix_embedding_path=word_char_mix_embedding_path)
#for i, dataset in datasets.items():`: 遍历数据集字典 `datasets` 中的每一项，
    # 其中 `i` 是键（通常是字符串类型的 `'train'`, `'dev'`, `'test'`），`dataset` 则是对应的数据集对象。
    for i, dataset in datasets.items():
        dataset.set_input('chars', 'bigrams', 'target')
        dataset.set_input('words')
  #     dataset.set_input('pos_s', 'pos_e', 'lex_s', 'lex_e'): 将 pos_s) (pos_e) (lex_s) (lex_e) 添加到输入数据中。
        #本身是在equip_chinese_ner_with_lexicon里得到的
        dataset.set_input('seq_len', 'lex_num')
        dataset.set_input('pos_s', 'pos_e', 'lex_s', 'lex_e')
        dataset.set_target('seq_len', 'target')
#计算所有数据集中序列长度的最大值。使用 `map` 函数将每个数据集中的序列长度提取出来，msra这个太大了能不能减小一点
    max_seq_len = max(*map(lambda x: max(x['seq_len']), datasets.values()))

    if use_bigram:
        bi_embed = embeddings['bigram']
    else:
        bi_embed = None
#注意，这里初始化的model只传入了lattice，还有bigram,没有char
    model = NFLAT(tag_vocab=vocabs['label'], char_embed=embeddings['lattice'], word_embed=embeddings['lattice'],
                  num_layers=num_layers, hidden_size=d_model, n_head=n_heads,head_dims=head_dims,
                  feedforward_dim=dim_feedforward, dropout=dropout, max_seq_len=max_seq_len,
                  after_norm=after_norm, attn_type=attn_type,
                  bi_embed=bi_embed,
                  char_dropout=char_embed_dropout,
                  word_dropout=word_embed_dropout,
                  fc_dropout=fc_dropout,
                  pos_embed=pos_embed,
                  scale=scale,
                  softmax_axis=softmax_axis,
                  vocab=vocabs['lattice'],
                  four_pos_fusion=four_pos_fusion,
                  before=before,
                  is_less_head=is_less_head,
                  attn_dropout=attn_dropout)
#参数数量初始化
    params_nums = 0
    #`n` 是参数的名称，`p` 是参数的值。这段代码的作用是计算模型中除了字符嵌入和双字符嵌入相关的参数外，其他参数的总数量。
    for n, p in model.named_parameters():
        print('{}:{}'.format(n, p.size()))
        if 'char_embed' not in n and 'bi_embed' not in n:
            x = 1
            for size in p.size():
                x *= size
            params_nums += x
    print('params_nums:', params_nums)
#上下文管理器，该上下文中的操作不会被记录在计算图中，用于节省内存和加快计算。
    with torch.no_grad():
        print('{}init pram{}'.format('*' * 15, '*' * 15))
        for n, p in model.named_parameters():
            # print(n, p.size())
            if 'embedding' not in n and 'pos' not in n and 'pe' not in n \
                    and 'bias' not in n and 'crf' not in n and p.dim() > 1:
                try:
                    if args.init == 'uniform':
                        nn.init.xavier_uniform_(p)
                        print('xavier uniform init:{}'.format(n))
                    elif args.init == 'norm':
                        print('xavier norm init:{}'.format(n))
                        nn.init.xavier_normal_(p)
                except Exception as e:
                    print(e)
                    print(n)
                    exit(1208)
        print('{}init pram{}'.format('*' * 15, '*' * 15))
#获取模型中 CRF 层的参数列表，crf_params第一次出现
    crf_params = list(model.crf.parameters())
    #获取 CRF 参数的 id 列表，注意所有模型的三层都在NFLAT,这里只是获取然后打印，或者下面要获取的参数
    crf_params_ids = list(map(id, crf_params))
    non_crf_params = list(filter(lambda x: id(x) not in crf_params_ids, model.parameters()))

    param_ = [{'params': non_crf_params}, {'params': crf_params, 'lr': lr * crf_lr}]

    optimizer = optim.SGD(param_, lr=lr, momentum=momentum)

    callbacks = []
#callback应该是fastnlp的一个py文件，不是我的
    lrschedule_callback = LRScheduler(lr_scheduler=LambdaLR(optimizer, lambda ep: 1 / (1 + 0.05 * ep)))
    clip_callback = GradientClipCallback(clip_type='value', clip_value=5)
    evaluate_callback = MyEvaluateCallback(datasets['test'])

    if warmup_steps > 0:
        warmup_callback = WarmupCallback(warmup_steps, schedule='linear')
        callbacks.append(warmup_callback)
    callbacks.extend([clip_callback, lrschedule_callback, evaluate_callback])

    print("-" * 20)
    print("Hyper-parameters")
    print("lexicon_name:", args.lexicon_name)
    print("n_heads:", n_heads)
    print("head_dims:", head_dims)
    print("num_layers:", num_layers)
    print("lr:", lr)
    print("attn_type:", attn_type)
    print("n_epochs:", n_epochs)
    print("batch_size:", batch_size)
    print("warmup_steps:", warmup_steps)
    print("model_type:", model_type)
    print("n_epochs:", n_epochs)
    print("momentum:", momentum)
    print("seed:", seed)
    print("-" * 20)

    print('parameter weight:')
    print(model.state_dict()['informer.layer_0.ffn.0.weight'])

    f1_metric = SpanFPreRecMetric(vocabs['label'], encoding_type=encoding_type)
    acc_metric = AccuracyMetric()
    acc_metric.set_metric_name('label_acc')
    metrics = [
        f1_metric,
        acc_metric
    ]
#trainer也是fastnlp的py文件不是我的
    trainer = Trainer(datasets['train'], model, optimizer, batch_size=batch_size, sampler=BucketSampler(),
                      num_workers=0, n_epochs=n_epochs, dev_data=datasets['dev'],
                      metrics=metrics,
                      dev_batch_size=batch_size, callbacks=callbacks, device=device, test_use_tqdm=False,
                      use_tqdm=True, print_every=3000, save_path=None)
    trainer.train(load_best_model=False)


if __name__ == '__main__':
    main()
